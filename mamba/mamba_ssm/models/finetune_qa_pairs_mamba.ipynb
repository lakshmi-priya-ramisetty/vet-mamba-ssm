{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "import numpy as np\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = '/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/retrain_checkpoints/checkpoint-62046'\n",
    "checkpoint_path = '/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/retrain_checkpoints/checkpoint-517075'\n",
    "config_path = \"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/checkpoints/checkpoint-3236/config.json\"\n",
    "\n",
    "config = MambaConfig.from_pretrained(config_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = MambaForCausalLM.from_pretrained(checkpoint_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/scratch/vetgpt/repo/MedVetGPT/qa_generate/0508_short2_nodigit/'\n",
    "\n",
    "datasets = load_dataset('json', data_files={\n",
    "    'train': dataset_dir + 'train.json',\n",
    "    'test': dataset_dir + 'test.json'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 558894/558894 [01:03<00:00, 8860.91 examples/s] \n",
      "Map: 100%|██████████| 62100/62100 [00:07<00:00, 8643.23 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    inputs = [q for q in examples[\"Question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = tokenizer(examples[\"Answer\"], max_length=128, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization to both train and test datasets\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558894"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"].num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    result[\"avg_rouge\"] = np.mean(list(result.values()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vetgpt/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-11 15:14:35,556] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/mamba_finetune_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir='/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/mamba_finetune_results/mamba_logs',\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-3,\n",
    "    disable_tqdm=False,\n",
    "    save_steps=500, \n",
    "    save_total_limit=3      \n",
    ")\n",
    "\n",
    "lora_config =  LoraConfig(\n",
    "        r=8,\n",
    "        target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        bias=\"none\"\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/62100 [00:00<?, ? examples/s]/tmp/ipykernel_401537/1489932227.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n",
      "Map: 100%|██████████| 62100/62100 [23:30<00:00, 44.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "checkpoint_path = \"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/mamba_finetune_results/checkpoint-100\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = MambaForCausalLM.from_pretrained(checkpoint_path)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_dir = '/scratch/vetgpt/repo/MedVetGPT/qa_generate/0508_short2_nodigit/'\n",
    "test_dataset = load_dataset('json', data_files={'test': dataset_dir + 'test.json'})['test']\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q for q in examples[\"Question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "def generate_predictions(batch):\n",
    "    inputs = tokenizer(batch[\"Question\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        outputs = model.generate(inputs, max_new_tokens=30, num_beams=1)\n",
    "    \n",
    "    batch[\"predictions\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "predicted_test_dataset = test_dataset.map(generate_predictions, batched=True, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model ROUGE-1 r ROUGE-1 p ROUGE-1 f ROUGE-2 r ROUGE-2 p ROUGE-2 f  \\\n",
      "0  MambaLMHead    0.2735    0.0701    0.1118    0.0369    0.0072    0.0122   \n",
      "\n",
      "  ROUGE-L r ROUGE-L p ROUGE-L f  \n",
      "0    0.2456    0.0614    0.0954  \n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = {'r': [], 'p': [], 'f': []}\n",
    "rouge2_scores = {'r': [], 'p': [], 'f': []}\n",
    "rougeL_scores = {'r': [], 'p': [], 'f': []}\n",
    "\n",
    "for pred, ref in zip(predicted_test_dataset[\"predictions\"], predicted_test_dataset[\"Answer\"]):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    rouge1_scores['r'].append(scores['rouge1'].recall)\n",
    "    rouge1_scores['p'].append(scores['rouge1'].precision)\n",
    "    rouge1_scores['f'].append(scores['rouge1'].fmeasure)\n",
    "    \n",
    "    rouge2_scores['r'].append(scores['rouge2'].recall)\n",
    "    rouge2_scores['p'].append(scores['rouge2'].precision)\n",
    "    rouge2_scores['f'].append(scores['rouge2'].fmeasure)\n",
    "    \n",
    "    rougeL_scores['r'].append(scores['rougeL'].recall)\n",
    "    rougeL_scores['p'].append(scores['rougeL'].precision)\n",
    "    rougeL_scores['f'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "rouge1_avg = {k: round(sum(v)/len(v), 4) for k, v in rouge1_scores.items()}\n",
    "rouge2_avg = {k: round(sum(v)/len(v), 4) for k, v in rouge2_scores.items()}\n",
    "rougeL_avg = {k: round(sum(v)/len(v), 4) for k, v in rougeL_scores.items()}\n",
    "\n",
    "data = {\n",
    "    \"Model\": [\"MambaLMHead\"],\n",
    "    \"ROUGE-1 r\": [rouge1_avg['r']],\n",
    "    \"ROUGE-1 p\": [rouge1_avg['p']],\n",
    "    \"ROUGE-1 f\": [rouge1_avg['f']],\n",
    "    \"ROUGE-2 r\": [rouge2_avg['r']],\n",
    "    \"ROUGE-2 p\": [rouge2_avg['p']],\n",
    "    \"ROUGE-2 f\": [rouge2_avg['f']],\n",
    "    \"ROUGE-L r\": [rougeL_avg['r']],\n",
    "    \"ROUGE-L p\": [rougeL_avg['p']],\n",
    "    \"ROUGE-L f\": [rougeL_avg['f']]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/rouge_scores.csv\", index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 62100 examples [00:00, 456686.77 examples/s]\n",
      "Map: 100%|██████████| 62100/62100 [00:03<00:00, 18697.87 examples/s]\n",
      "Map:   0%|          | 0/62100 [00:00<?, ? examples/s]/tmp/ipykernel_3152841/3736962619.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n",
      "Map: 100%|██████████| 62100/62100 [11:58<00:00, 86.38 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# 1.4b checkpoint\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "checkpoint_path = \"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/mamba_finetune_results/checkpoint-600\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = MambaForCausalLM.from_pretrained(checkpoint_path)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_dir = '/scratch/vetgpt/repo/MedVetGPT/qa_generate/0508_short2_nodigit/'\n",
    "test_dataset = load_dataset('json', data_files={'test': dataset_dir + 'test.json'})['test']\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q for q in examples[\"Question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "def generate_predictions(batch):\n",
    "    inputs = tokenizer(batch[\"Question\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        outputs = model.generate(inputs, max_new_tokens=30, num_beams=1)\n",
    "    \n",
    "    batch[\"predictions\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "predicted_test_dataset = test_dataset.map(generate_predictions, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model ROUGE-1 r ROUGE-1 p ROUGE-1 f ROUGE-2 r ROUGE-2 p ROUGE-2 f  \\\n",
      "0  MambaCausalLM    0.3127    0.0863    0.1353    0.0489    0.0107    0.0176   \n",
      "\n",
      "  ROUGE-L r ROUGE-L p ROUGE-L f  \n",
      "0    0.2783    0.0732    0.1159  \n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = {'r': [], 'p': [], 'f': []}\n",
    "rouge2_scores = {'r': [], 'p': [], 'f': []}\n",
    "rougeL_scores = {'r': [], 'p': [], 'f': []}\n",
    "\n",
    "for pred, ref in zip(predicted_test_dataset[\"predictions\"], predicted_test_dataset[\"Answer\"]):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    rouge1_scores['r'].append(scores['rouge1'].recall)\n",
    "    rouge1_scores['p'].append(scores['rouge1'].precision)\n",
    "    rouge1_scores['f'].append(scores['rouge1'].fmeasure)\n",
    "    \n",
    "    rouge2_scores['r'].append(scores['rouge2'].recall)\n",
    "    rouge2_scores['p'].append(scores['rouge2'].precision)\n",
    "    rouge2_scores['f'].append(scores['rouge2'].fmeasure)\n",
    "    \n",
    "    rougeL_scores['r'].append(scores['rougeL'].recall)\n",
    "    rougeL_scores['p'].append(scores['rougeL'].precision)\n",
    "    rougeL_scores['f'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "rouge1_avg = {k: round(sum(v)/len(v), 4) for k, v in rouge1_scores.items()}\n",
    "rouge2_avg = {k: round(sum(v)/len(v), 4) for k, v in rouge2_scores.items()}\n",
    "rougeL_avg = {k: round(sum(v)/len(v), 4) for k, v in rougeL_scores.items()}\n",
    "\n",
    "data = {\n",
    "    \"Model\": [\"MambaCausalLM\"],\n",
    "    \"ROUGE-1 r\": [rouge1_avg['r']],\n",
    "    \"ROUGE-1 p\": [rouge1_avg['p']],\n",
    "    \"ROUGE-1 f\": [rouge1_avg['f']],\n",
    "    \"ROUGE-2 r\": [rouge2_avg['r']],\n",
    "    \"ROUGE-2 p\": [rouge2_avg['p']],\n",
    "    \"ROUGE-2 f\": [rouge2_avg['f']],\n",
    "    \"ROUGE-L r\": [rougeL_avg['r']],\n",
    "    \"ROUGE-L p\": [rougeL_avg['p']],\n",
    "    \"ROUGE-L f\": [rougeL_avg['f']]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/rouge_scores_2.csv\", index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/62100 [00:00<?, ? examples/s]/tmp/ipykernel_858669/3736962619.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n",
      "Map: 100%|██████████| 62100/62100 [10:37<00:00, 97.45 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# 1.4b checkpoint\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "checkpoint_path = \"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/mamba_finetune_results/checkpoint-600\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = MambaForCausalLM.from_pretrained(checkpoint_path)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_dir = '/scratch/vetgpt/repo/MedVetGPT/qa_generate/0508_short2_nodigit/'\n",
    "test_dataset = load_dataset('json', data_files={'test': dataset_dir + 'test.json'})['test']\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q for q in examples[\"Question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "def generate_predictions(batch):\n",
    "    inputs = tokenizer(batch[\"Question\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        outputs = model.generate(inputs, max_new_tokens=30, num_beams=1)\n",
    "    \n",
    "    batch[\"predictions\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "predicted_test_dataset = test_dataset.map(generate_predictions, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_909890/1172566956.py:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are dogs usually capable of resolving quickly with toys?\n",
      "Prediction: What are dogs usually capable of resolving quickly with toys? What are they not? What are the most common toys that dogs like to play with? What are the most common toys that dogs like to play with\n"
     ]
    }
   ],
   "source": [
    "def test_on_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        outputs = model.generate(inputs, max_new_tokens=30, num_beams=1)\n",
    "    \n",
    "    predictions = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return predictions\n",
    "\n",
    "example_question = \"What are dogs usually capable of resolving quickly with toys?\"\n",
    "prediction = test_on_text(example_question)\n",
    "print(f\"Question: {example_question}\")\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vetgpt/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is contaminated with rodent droppings? The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"azsxscdvfb/VetMedGPT-1B-chat-V0.2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"azsxscdvfb/VetMedGPT-1B-chat-V0.2\")\n",
    "\n",
    "prompt = \"What is contaminated with rodent droppings?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=24, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are dogs usually capable of resolving quickly with toys? The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are dogs usually capable of resolving quickly with toys?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=50, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4b checkpoint\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "checkpoint_path = '/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/retrain_checkpoints/checkpoint-517075'\n",
    "# from transformers import AutoConfig\n",
    "\n",
    "# config = AutoConfig.from_pretrained(checkpoint_path)\n",
    "config_path = \"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/checkpoints/checkpoint-3236/config.json\"\n",
    "\n",
    "config = MambaConfig.from_pretrained(config_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = MambaForCausalLM.from_pretrained(checkpoint_path, config=config, ignore_mismatched_sizes=True)\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 62100/62100 [00:04<00:00, 13841.50 examples/s]\n",
      "Map:   0%|          | 0/62100 [00:00<?, ? examples/s]/tmp/ipykernel_909890/2355121210.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n",
      "Map: 100%|██████████| 62100/62100 [11:35<00:00, 89.29 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# 1.4b checkpoint\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_dir = '/scratch/vetgpt/repo/MedVetGPT/qa_generate/0508_short2_nodigit/'\n",
    "test_dataset = load_dataset('json', data_files={'test': dataset_dir + 'test.json'})['test']\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q for q in examples[\"Question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "def generate_predictions(batch):\n",
    "    inputs = tokenizer(batch[\"Question\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        outputs = model.generate(inputs, max_new_tokens=30, num_beams=1)\n",
    "    \n",
    "    batch[\"predictions\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "predicted_test_dataset = test_dataset.map(generate_predictions, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  ROUGE-1 r  ROUGE-1 p  ROUGE-1 f  ROUGE-2 r  ROUGE-2 p  \\\n",
      "0  MambaCausalLM     0.3836     0.1706     0.2216     0.2134     0.0868   \n",
      "\n",
      "   ROUGE-2 f  ROUGE-L r  ROUGE-L p  ROUGE-L f  \n",
      "0     0.1169     0.3314     0.1428     0.1875  \n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = {'r': [], 'p': [], 'f': []}\n",
    "rouge2_scores = {'r': [], 'p': [], 'f': []}\n",
    "rougeL_scores = {'r': [], 'p': [], 'f': []}\n",
    "\n",
    "for pred, ref in zip(predicted_test_dataset[\"predictions\"], predicted_test_dataset[\"Answer\"]):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    rouge1_scores['r'].append(scores['rouge1'].recall)\n",
    "    rouge1_scores['p'].append(scores['rouge1'].precision)\n",
    "    rouge1_scores['f'].append(scores['rouge1'].fmeasure)\n",
    "    \n",
    "    rouge2_scores['r'].append(scores['rouge2'].recall)\n",
    "    rouge2_scores['p'].append(scores['rouge2'].precision)\n",
    "    rouge2_scores['f'].append(scores['rouge2'].fmeasure)\n",
    "    \n",
    "    rougeL_scores['r'].append(scores['rougeL'].recall)\n",
    "    rougeL_scores['p'].append(scores['rougeL'].precision)\n",
    "    rougeL_scores['f'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "rouge1_avg = {k: round(sum(v)/len(v), 4) for k, v in rouge1_scores.items()}\n",
    "rouge2_avg = {k: round(sum(v)/len(v), 4) for k, v in rouge2_scores.items()}\n",
    "rougeL_avg = {k: round(sum(v)/len(v), 4) for k, v in rougeL_scores.items()}\n",
    "\n",
    "data = {\n",
    "    \"Model\": [\"MambaCausalLM\"],\n",
    "    \"ROUGE-1 r\": [rouge1_avg['r']],\n",
    "    \"ROUGE-1 p\": [rouge1_avg['p']],\n",
    "    \"ROUGE-1 f\": [rouge1_avg['f']],\n",
    "    \"ROUGE-2 r\": [rouge2_avg['r']],\n",
    "    \"ROUGE-2 p\": [rouge2_avg['p']],\n",
    "    \"ROUGE-2 f\": [rouge2_avg['f']],\n",
    "    \"ROUGE-L r\": [rougeL_avg['r']],\n",
    "    \"ROUGE-L p\": [rougeL_avg['p']],\n",
    "    \"ROUGE-L f\": [rougeL_avg['f']]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/rouge_scores_4.csv\", index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the most common type of human dendritic cell?\\n\\nDendritic cells are the most common type of immune cell']\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"What is the most common type of human dendritic cell?\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "out = model.generate(input_ids, max_new_tokens=14)\n",
    "\n",
    "print(tokenizer.batch_decode(out, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is contaminated with rodent droppings?\n",
      "\n",
      "The answer is that it is contaminated with rodent droppings.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is contaminated with rodent droppings?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=24, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the name of the indigenous Cerrado?\n",
      "\n",
      "The Cerrado is a large area of tropical rainforest in the south of Brazil\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the name of the indigenous Cerrado?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=30, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the most infected animal in the UK?\n",
      "\n",
      "The most infected animal in the UK is the dog.\n",
      "\n",
      "What is the most infected\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the most infected animal in the UK?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=30, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the area of the mouth that your dog has bad breath?\n",
      "\n",
      "A:\n",
      "\n",
      "The area of the mouth that your dog has bad breath is the area of the mouth that is covered by the tongue.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the area of the mouth that your dog has bad breath?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=44, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My dog cannot stand, what is the reason?\n",
      "\n",
      "A:\n",
      "\n",
      "The reason is that your dog is not a dog.\n",
      "\n",
      "A:\n",
      "\n",
      "The reason is that your dog is not a dog.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"My dog cannot stand, what is the reason?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=44, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My cat is not standing still.\n",
      "\n",
      "I am not sure if I am being a little bit too harsh on the cat, but I am not sure if I am being a little bit too harsh on the cat.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"My cat is not standing\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=44, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain why my cat cannot stand the smell of the food.\n",
      "\n",
      "Answer:\n",
      "\n",
      "The cat is not able to smell the food because it is not in the same room as the food. The cat is not\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain why my cat cannot stand\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=44, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning with High-Quality Data\n",
    "# Instruction tuning - fine-tune the model on datasets where instructions or questions are mapped to relevant responses, helping the model generate focused answers.\n",
    "# Guiding examples in the prompt to get desired behaviour\n",
    "# Incorporate RAG on some faq's, context and use my model as generator?\n",
    "# Feedback loops - regularly retrain the model using real-world interactions or feedback to improve the accuracy of the responses.\n",
    "# Mamba struggles with coherence if it lacks prior context. Use longer sequences during fine-tuning to capture context across multiple turns of dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4044 examples [00:00, 182559.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "test_dir = '/scratch/vetgpt/vetgpt-rlp/mamba/qa-pairs/train.json'\n",
    "\n",
    "test = load_dataset('json', data_files={\n",
    "    'train': test_dir\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Question', 'Answer'],\n",
       "        num_rows: 4044\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(50280, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x MambaBlock(\n",
       "        (norm): MambaRMSNorm(2048, eps=1e-05)\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(3,), groups=4096)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (x_proj): Linear(in_features=4096, out_features=160, bias=False)\n",
       "          (dt_proj): Linear(in_features=128, out_features=4096, bias=True)\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm(2048, eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How often should I vaccinate my cat?\n",
      "\n",
      "The American Kennel Club (AKC) recommends vaccinating your cat every six months. The AKC recommends vaccinating your cat every three months.\n",
      "\n",
      "What is the best way to vaccinate my cat?\n",
      "\n",
      "The best way to vaccinate your cat\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How often should I vaccinate my cat?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can pets transmit Lyme disease to humans?\n",
      "\n",
      "Lyme disease is a tick-borne disease that can be spread by ticks. It is transmitted by the bite of a tick that carries the spirochete Borrelia burgdorferi.\n",
      "\n",
      "The tick is a member of the family Ix\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can pets transmit Lyme disease to humans?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the best diet for a growing puppy?\n",
      "\n",
      "A:\n",
      "\n",
      "I would recommend a low-carb diet.  This is because the puppy is growing at a very rapid rate and the body needs to be able to use the energy it is getting from the food.  The puppy will need to eat a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the best diet for a growing puppy?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the common complications during animal labor?\n",
      "\n",
      "The most common complications during labor are as follows:\n",
      "\n",
      "• Fetal distress\n",
      "\n",
      "• Fetal distress\n",
      "\n",
      "• Fetal distress\n",
      "\n",
      "• Fetal distress\n",
      "\n",
      "• Fetal distress\n",
      "\n",
      "• Fetal distress\n",
      "\n",
      "• Fetal distress\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are the common complications during animal labor?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can I give my dog homemade food instead of kibble?\n",
      "\n",
      "Yes, but you have to be careful. If you give your dog homemade food, you have to make sure that it is safe for your dog to eat. If you give your dog homemade food, you have to make sure that it is safe\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can I give my dog homemade food instead of kibble?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can turtles get respiratory infections?\n",
      "\n",
      "Turtles can get respiratory infections.\n",
      "\n",
      "Turtles can get respiratory infections.\n",
      "\n",
      "Turtles can get respiratory infections.\n",
      "\n",
      "Turtles can get respiratory infections.\n",
      "\n",
      "Turtles can get respiratory infections.\n",
      "\n",
      "Turtles can get\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can turtles get respiratory infections?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When is vomiting in cats considered an emergency?\n",
      "\n",
      "Vomiting is a common sign of an emergency in cats. It is a sign of dehydration, which is a common cause of vomiting in cats.\n",
      "\n",
      "What is the difference between vomiting and diarrhea?\n",
      "\n",
      "Vomiting is the loss of fluid from the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"When is vomiting in cats considered an emergency?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I stop bleeding from a pet's wound?\n",
      "\n",
      "A:\n",
      "\n",
      "The best way to stop bleeding is to stop the bleeding.\n",
      "If you have a pet that is bleeding from a wound, you should immediately stop the bleeding.\n",
      "If you have a pet that is bleeding from a wound, you should\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How do I stop bleeding from a pet's wound?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
