{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "import numpy as np\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = '/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/retrain_checkpoints/checkpoint-62046'\n",
    "checkpoint_path = '/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/retrain_checkpoints/checkpoint-517075'\n",
    "config_path = \"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/checkpoints/checkpoint-3236/config.json\"\n",
    "\n",
    "config = MambaConfig.from_pretrained(config_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = MambaForCausalLM.from_pretrained(checkpoint_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/scratch/vetgpt/repo/MedVetGPT/qa_generate/0508_short2_nodigit/'\n",
    "\n",
    "datasets = load_dataset('json', data_files={\n",
    "    'train': dataset_dir + 'train.json',\n",
    "    'test': dataset_dir + 'test.json'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 558894/558894 [01:03<00:00, 8860.91 examples/s] \n",
      "Map: 100%|██████████| 62100/62100 [00:07<00:00, 8643.23 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    inputs = [q for q in examples[\"Question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = tokenizer(examples[\"Answer\"], max_length=128, truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization to both train and test datasets\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558894"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"].num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "\n",
    "    result[\"avg_rouge\"] = np.mean(list(result.values()))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vetgpt/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-11 15:14:35,556] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/mamba_finetune_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir='/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/mamba_finetune_results/mamba_logs',\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-3,\n",
    "    disable_tqdm=False,\n",
    "    save_steps=500, \n",
    "    save_total_limit=3      \n",
    ")\n",
    "\n",
    "lora_config =  LoraConfig(\n",
    "        r=8,\n",
    "        target_modules=[\"x_proj\", \"embeddings\", \"in_proj\", \"out_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        bias=\"none\"\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 30.19 MiB is free. Process 238946 has 31.69 GiB memory in use. Process 1389993 has 32.89 GiB memory in use. Including non-PyTorch memory, this process has 14.52 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 86.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/transformers/trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/transformers/trainer.py:2474\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2474\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2477\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2479\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2480\u001b[0m ):\n\u001b[1;32m   2481\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/transformers/trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3571\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3572\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3578\u001b[0m ):\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/transformers/trainer.py:3625\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3623\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3624\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3625\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3626\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/peft/peft_model.py:1430\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1429\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1430\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1432\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1435\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1438\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1441\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:179\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:775\u001b[0m, in \u001b[0;36mMambaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, cache_params, labels, output_hidden_states, return_dict, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    773\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 775\u001b[0m mamba_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    785\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m mamba_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    787\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype))\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:627\u001b[0m, in \u001b[0;36mMambaModel.forward\u001b[0;34m(self, input_ids, inputs_embeds, cache_params, use_cache, output_hidden_states, return_dict, cache_position, attention_mask)\u001b[0m\n\u001b[1;32m    623\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    624\u001b[0m         mixer_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, hidden_states, cache_params, cache_position, attention_mask\n\u001b[1;32m    625\u001b[0m     )\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 627\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmixer_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    635\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:371\u001b[0m, in \u001b[0;36mMambaBlock.forward\u001b[0;34m(self, hidden_states, cache_params, cache_position, attention_mask)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_in_fp32:\n\u001b[1;32m    369\u001b[0m     residual \u001b[38;5;241m=\u001b[39m residual\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m--> 371\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:327\u001b[0m, in \u001b[0;36mMambaMixer.forward\u001b[0;34m(self, hidden_states, cache_params, cache_position, attention_mask)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_fast_path_available \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_proj\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mis_compiling():\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask)\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslow_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vetgpt/.venv/lib/python3.10/site-packages/transformers/models/mamba/modeling_mamba.py:291\u001b[0m, in \u001b[0;36mMambaMixer.slow_forward\u001b[0;34m(self, input_states, cache_params, cache_position, attention_mask)\u001b[0m\n\u001b[1;32m    289\u001b[0m discrete_A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(A[\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m, :] \u001b[38;5;241m*\u001b[39m discrete_time_step[:, :, :, \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;66;03m# [batch, intermediate_size, seq_len, ssm_state_size]\u001b[39;00m\n\u001b[1;32m    290\u001b[0m discrete_B \u001b[38;5;241m=\u001b[39m discrete_time_step[:, :, :, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m B[:, \u001b[38;5;28;01mNone\u001b[39;00m, :, :]\u001b[38;5;241m.\u001b[39mfloat()       \u001b[38;5;66;03m# [batch, intermediate_size, seq_len, ssm_state_size]\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m deltaB_u \u001b[38;5;241m=\u001b[39m \u001b[43mdiscrete_B\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# 3.c perform the recurrence y ← SSM(A, B, C)(x)\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_mambapy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m cache_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 30.19 MiB is free. Process 238946 has 31.69 GiB memory in use. Process 1389993 has 32.89 GiB memory in use. Including non-PyTorch memory, this process has 14.52 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 86.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/62100 [00:00<?, ? examples/s]/tmp/ipykernel_401537/1489932227.py:13: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n",
      "Map: 100%|██████████| 62100/62100 [23:30<00:00, 44.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "checkpoint_path = \"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/mamba_finetune_results/checkpoint-100\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = MambaForCausalLM.from_pretrained(checkpoint_path)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_dir = '/scratch/vetgpt/repo/MedVetGPT/qa_generate/0508_short2_nodigit/'\n",
    "test_dataset = load_dataset('json', data_files={'test': dataset_dir + 'test.json'})['test']\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q for q in examples[\"Question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "def generate_predictions(batch):\n",
    "    inputs = tokenizer(batch[\"Question\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        outputs = model.generate(inputs, max_new_tokens=30, num_beams=1)\n",
    "    \n",
    "    batch[\"predictions\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "predicted_test_dataset = test_dataset.map(generate_predictions, batched=True, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model ROUGE-1 r ROUGE-1 p ROUGE-1 f ROUGE-2 r ROUGE-2 p ROUGE-2 f  \\\n",
      "0  MambaLMHead    0.2735    0.0701    0.1118    0.0369    0.0072    0.0122   \n",
      "\n",
      "  ROUGE-L r ROUGE-L p ROUGE-L f  \n",
      "0    0.2456    0.0614    0.0954  \n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = {'r': [], 'p': [], 'f': []}\n",
    "rouge2_scores = {'r': [], 'p': [], 'f': []}\n",
    "rougeL_scores = {'r': [], 'p': [], 'f': []}\n",
    "\n",
    "for pred, ref in zip(predicted_test_dataset[\"predictions\"], predicted_test_dataset[\"Answer\"]):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    rouge1_scores['r'].append(scores['rouge1'].recall)\n",
    "    rouge1_scores['p'].append(scores['rouge1'].precision)\n",
    "    rouge1_scores['f'].append(scores['rouge1'].fmeasure)\n",
    "    \n",
    "    rouge2_scores['r'].append(scores['rouge2'].recall)\n",
    "    rouge2_scores['p'].append(scores['rouge2'].precision)\n",
    "    rouge2_scores['f'].append(scores['rouge2'].fmeasure)\n",
    "    \n",
    "    rougeL_scores['r'].append(scores['rougeL'].recall)\n",
    "    rougeL_scores['p'].append(scores['rougeL'].precision)\n",
    "    rougeL_scores['f'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "rouge1_avg = {k: round(sum(v)/len(v), 4) for k, v in rouge1_scores.items()}\n",
    "rouge2_avg = {k: round(sum(v)/len(v), 4) for k, v in rouge2_scores.items()}\n",
    "rougeL_avg = {k: round(sum(v)/len(v), 4) for k, v in rougeL_scores.items()}\n",
    "\n",
    "data = {\n",
    "    \"Model\": [\"MambaLMHead\"],\n",
    "    \"ROUGE-1 r\": [rouge1_avg['r']],\n",
    "    \"ROUGE-1 p\": [rouge1_avg['p']],\n",
    "    \"ROUGE-1 f\": [rouge1_avg['f']],\n",
    "    \"ROUGE-2 r\": [rouge2_avg['r']],\n",
    "    \"ROUGE-2 p\": [rouge2_avg['p']],\n",
    "    \"ROUGE-2 f\": [rouge2_avg['f']],\n",
    "    \"ROUGE-L r\": [rougeL_avg['r']],\n",
    "    \"ROUGE-L p\": [rougeL_avg['p']],\n",
    "    \"ROUGE-L f\": [rougeL_avg['f']]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/rouge_scores.csv\", index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 62100 examples [00:00, 456686.77 examples/s]\n",
      "Map: 100%|██████████| 62100/62100 [00:03<00:00, 18697.87 examples/s]\n",
      "Map:   0%|          | 0/62100 [00:00<?, ? examples/s]/tmp/ipykernel_3152841/3736962619.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n",
      "Map: 100%|██████████| 62100/62100 [11:58<00:00, 86.38 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# 1.4b checkpoint\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "checkpoint_path = \"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/mamba_finetune_results/checkpoint-600\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = MambaForCausalLM.from_pretrained(checkpoint_path)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_dir = '/scratch/vetgpt/repo/MedVetGPT/qa_generate/0508_short2_nodigit/'\n",
    "test_dataset = load_dataset('json', data_files={'test': dataset_dir + 'test.json'})['test']\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q for q in examples[\"Question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "def generate_predictions(batch):\n",
    "    inputs = tokenizer(batch[\"Question\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        outputs = model.generate(inputs, max_new_tokens=30, num_beams=1)\n",
    "    \n",
    "    batch[\"predictions\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "predicted_test_dataset = test_dataset.map(generate_predictions, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model ROUGE-1 r ROUGE-1 p ROUGE-1 f ROUGE-2 r ROUGE-2 p ROUGE-2 f  \\\n",
      "0  MambaCausalLM    0.3127    0.0863    0.1353    0.0489    0.0107    0.0176   \n",
      "\n",
      "  ROUGE-L r ROUGE-L p ROUGE-L f  \n",
      "0    0.2783    0.0732    0.1159  \n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = {'r': [], 'p': [], 'f': []}\n",
    "rouge2_scores = {'r': [], 'p': [], 'f': []}\n",
    "rougeL_scores = {'r': [], 'p': [], 'f': []}\n",
    "\n",
    "for pred, ref in zip(predicted_test_dataset[\"predictions\"], predicted_test_dataset[\"Answer\"]):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    rouge1_scores['r'].append(scores['rouge1'].recall)\n",
    "    rouge1_scores['p'].append(scores['rouge1'].precision)\n",
    "    rouge1_scores['f'].append(scores['rouge1'].fmeasure)\n",
    "    \n",
    "    rouge2_scores['r'].append(scores['rouge2'].recall)\n",
    "    rouge2_scores['p'].append(scores['rouge2'].precision)\n",
    "    rouge2_scores['f'].append(scores['rouge2'].fmeasure)\n",
    "    \n",
    "    rougeL_scores['r'].append(scores['rougeL'].recall)\n",
    "    rougeL_scores['p'].append(scores['rougeL'].precision)\n",
    "    rougeL_scores['f'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "rouge1_avg = {k: round(sum(v)/len(v), 4) for k, v in rouge1_scores.items()}\n",
    "rouge2_avg = {k: round(sum(v)/len(v), 4) for k, v in rouge2_scores.items()}\n",
    "rougeL_avg = {k: round(sum(v)/len(v), 4) for k, v in rougeL_scores.items()}\n",
    "\n",
    "data = {\n",
    "    \"Model\": [\"MambaCausalLM\"],\n",
    "    \"ROUGE-1 r\": [rouge1_avg['r']],\n",
    "    \"ROUGE-1 p\": [rouge1_avg['p']],\n",
    "    \"ROUGE-1 f\": [rouge1_avg['f']],\n",
    "    \"ROUGE-2 r\": [rouge2_avg['r']],\n",
    "    \"ROUGE-2 p\": [rouge2_avg['p']],\n",
    "    \"ROUGE-2 f\": [rouge2_avg['f']],\n",
    "    \"ROUGE-L r\": [rougeL_avg['r']],\n",
    "    \"ROUGE-L p\": [rougeL_avg['p']],\n",
    "    \"ROUGE-L f\": [rougeL_avg['f']]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/rouge_scores_2.csv\", index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/62100 [00:00<?, ? examples/s]/tmp/ipykernel_858669/3736962619.py:26: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n",
      "Map: 100%|██████████| 62100/62100 [10:37<00:00, 97.45 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# 1.4b checkpoint\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "checkpoint_path = \"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/mamba_finetune_results/checkpoint-600\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = MambaForCausalLM.from_pretrained(checkpoint_path)\n",
    "model.to(\"cuda\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_dir = '/scratch/vetgpt/repo/MedVetGPT/qa_generate/0508_short2_nodigit/'\n",
    "test_dataset = load_dataset('json', data_files={'test': dataset_dir + 'test.json'})['test']\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q for q in examples[\"Question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "def generate_predictions(batch):\n",
    "    inputs = tokenizer(batch[\"Question\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        outputs = model.generate(inputs, max_new_tokens=30, num_beams=1)\n",
    "    \n",
    "    batch[\"predictions\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "predicted_test_dataset = test_dataset.map(generate_predictions, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_909890/1172566956.py:5: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are dogs usually capable of resolving quickly with toys?\n",
      "Prediction: What are dogs usually capable of resolving quickly with toys? What are they not? What are the most common toys that dogs like to play with? What are the most common toys that dogs like to play with\n"
     ]
    }
   ],
   "source": [
    "def test_on_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        outputs = model.generate(inputs, max_new_tokens=30, num_beams=1)\n",
    "    \n",
    "    predictions = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return predictions\n",
    "\n",
    "example_question = \"What are dogs usually capable of resolving quickly with toys?\"\n",
    "prediction = test_on_text(example_question)\n",
    "print(f\"Question: {example_question}\")\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vetgpt/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Both `max_new_tokens` (=512) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is contaminated with rodent droppings? The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent. The most common rodent that can be found in the home is the most common rodent\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"azsxscdvfb/VetMedGPT-1B-chat-V0.2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"azsxscdvfb/VetMedGPT-1B-chat-V0.2\")\n",
    "\n",
    "prompt = \"What is contaminated with rodent droppings?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=24, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are dogs usually capable of resolving quickly with toys? The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat is a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most common cause of a dog's inability to eat. The most\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are dogs usually capable of resolving quickly with toys?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=50, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vetgpt/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)` is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-370m-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"state-spaces/mamba-370m-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4b checkpoint\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "checkpoint_path = '/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/retrain_checkpoints/checkpoint-517075'\n",
    "# from transformers import AutoConfig\n",
    "\n",
    "# config = AutoConfig.from_pretrained(checkpoint_path)\n",
    "config_path = \"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/checkpoints/checkpoint-3236/config.json\"\n",
    "\n",
    "config = MambaConfig.from_pretrained(config_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "model = MambaForCausalLM.from_pretrained(checkpoint_path, config=config, ignore_mismatched_sizes=True)\n",
    "\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 62100/62100 [00:04<00:00, 13841.50 examples/s]\n",
      "Map:   0%|          | 0/62100 [00:00<?, ? examples/s]/tmp/ipykernel_909890/2355121210.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast():\n",
      "Map: 100%|██████████| 62100/62100 [11:35<00:00, 89.29 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# 1.4b checkpoint\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_dir = '/scratch/vetgpt/repo/MedVetGPT/qa_generate/0508_short2_nodigit/'\n",
    "test_dataset = load_dataset('json', data_files={'test': dataset_dir + 'test.json'})['test']\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [q for q in examples[\"Question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "def generate_predictions(batch):\n",
    "    inputs = tokenizer(batch[\"Question\"], return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad(), autocast():\n",
    "        outputs = model.generate(inputs, max_new_tokens=30, num_beams=1)\n",
    "    \n",
    "    batch[\"predictions\"] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "predicted_test_dataset = test_dataset.map(generate_predictions, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  ROUGE-1 r  ROUGE-1 p  ROUGE-1 f  ROUGE-2 r  ROUGE-2 p  \\\n",
      "0  MambaCausalLM     0.3836     0.1706     0.2216     0.2134     0.0868   \n",
      "\n",
      "   ROUGE-2 f  ROUGE-L r  ROUGE-L p  ROUGE-L f  \n",
      "0     0.1169     0.3314     0.1428     0.1875  \n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import pandas as pd\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = {'r': [], 'p': [], 'f': []}\n",
    "rouge2_scores = {'r': [], 'p': [], 'f': []}\n",
    "rougeL_scores = {'r': [], 'p': [], 'f': []}\n",
    "\n",
    "for pred, ref in zip(predicted_test_dataset[\"predictions\"], predicted_test_dataset[\"Answer\"]):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    rouge1_scores['r'].append(scores['rouge1'].recall)\n",
    "    rouge1_scores['p'].append(scores['rouge1'].precision)\n",
    "    rouge1_scores['f'].append(scores['rouge1'].fmeasure)\n",
    "    \n",
    "    rouge2_scores['r'].append(scores['rouge2'].recall)\n",
    "    rouge2_scores['p'].append(scores['rouge2'].precision)\n",
    "    rouge2_scores['f'].append(scores['rouge2'].fmeasure)\n",
    "    \n",
    "    rougeL_scores['r'].append(scores['rougeL'].recall)\n",
    "    rougeL_scores['p'].append(scores['rougeL'].precision)\n",
    "    rougeL_scores['f'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "rouge1_avg = {k: round(sum(v)/len(v), 4) for k, v in rouge1_scores.items()}\n",
    "rouge2_avg = {k: round(sum(v)/len(v), 4) for k, v in rouge2_scores.items()}\n",
    "rougeL_avg = {k: round(sum(v)/len(v), 4) for k, v in rougeL_scores.items()}\n",
    "\n",
    "data = {\n",
    "    \"Model\": [\"MambaCausalLM\"],\n",
    "    \"ROUGE-1 r\": [rouge1_avg['r']],\n",
    "    \"ROUGE-1 p\": [rouge1_avg['p']],\n",
    "    \"ROUGE-1 f\": [rouge1_avg['f']],\n",
    "    \"ROUGE-2 r\": [rouge2_avg['r']],\n",
    "    \"ROUGE-2 p\": [rouge2_avg['p']],\n",
    "    \"ROUGE-2 f\": [rouge2_avg['f']],\n",
    "    \"ROUGE-L r\": [rougeL_avg['r']],\n",
    "    \"ROUGE-L p\": [rougeL_avg['p']],\n",
    "    \"ROUGE-L f\": [rougeL_avg['f']]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"/scratch/vetgpt/vetgpt-rlp/mamba/mamba_ssm/rouge_scores_4.csv\", index=False, float_format=\"%.4f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is the most common type of human dendritic cell?\\n\\nDendritic cells are the most common type of immune cell']\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"What is the most common type of human dendritic cell?\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "out = model.generate(input_ids, max_new_tokens=14)\n",
    "\n",
    "print(tokenizer.batch_decode(out, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is contaminated with rodent droppings?\n",
      "\n",
      "The answer is that it is contaminated with rodent droppings.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is contaminated with rodent droppings?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=24, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the name of the indigenous Cerrado?\n",
      "\n",
      "The Cerrado is a large area of tropical rainforest in the south of Brazil\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the name of the indigenous Cerrado?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=30, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the most infected animal in the UK?\n",
      "\n",
      "The most infected animal in the UK is the dog.\n",
      "\n",
      "What is the most infected\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the most infected animal in the UK?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=30, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the area of the mouth that your dog has bad breath?\n",
      "\n",
      "A:\n",
      "\n",
      "The area of the mouth that your dog has bad breath is the area of the mouth that is covered by the tongue.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the area of the mouth that your dog has bad breath?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=44, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My dog cannot stand, what is the reason?\n",
      "\n",
      "A:\n",
      "\n",
      "The reason is that your dog is not a dog.\n",
      "\n",
      "A:\n",
      "\n",
      "The reason is that your dog is not a dog.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"My dog cannot stand, what is the reason?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=44, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My cat is not standing still.\n",
      "\n",
      "I am not sure if I am being a little bit too harsh on the cat, but I am not sure if I am being a little bit too harsh on the cat.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"My cat is not standing\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=44, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain why my cat cannot stand the smell of the food.\n",
      "\n",
      "Answer:\n",
      "\n",
      "The cat is not able to smell the food because it is not in the same room as the food. The cat is not\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain why my cat cannot stand\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=44, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning with High-Quality Data\n",
    "# Instruction tuning - fine-tune the model on datasets where instructions or questions are mapped to relevant responses, helping the model generate focused answers.\n",
    "# Guiding examples in the prompt to get desired behaviour\n",
    "# Incorporate RAG on some faq's, context and use my model as generator?\n",
    "# Feedback loops - regularly retrain the model using real-world interactions or feedback to improve the accuracy of the responses.\n",
    "# Mamba struggles with coherence if it lacks prior context. Use longer sequences during fine-tuning to capture context across multiple turns of dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4044 examples [00:00, 182559.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "test_dir = '/scratch/vetgpt/vetgpt-rlp/mamba/qa-pairs/train.json'\n",
    "\n",
    "test = load_dataset('json', data_files={\n",
    "    'train': test_dir\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Question', 'Answer'],\n",
       "        num_rows: 4044\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(50280, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-47): 48 x MambaBlock(\n",
       "        (norm): MambaRMSNorm(2048, eps=1e-05)\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(3,), groups=4096)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (x_proj): Linear(in_features=4096, out_features=160, bias=False)\n",
       "          (dt_proj): Linear(in_features=128, out_features=4096, bias=True)\n",
       "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm(2048, eps=1e-05)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How often should I vaccinate my cat?\n",
      "\n",
      "The American Kennel Club (AKC) recommends vaccinating your cat every six months. The AKC recommends vaccinating your cat every three months.\n",
      "\n",
      "What is the best way to vaccinate my cat?\n",
      "\n",
      "The best way to vaccinate your cat\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How often should I vaccinate my cat?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can pets transmit Lyme disease to humans?\n",
      "\n",
      "Lyme disease is a tick-borne disease that can be spread by ticks. It is transmitted by the bite of a tick that carries the spirochete Borrelia burgdorferi.\n",
      "\n",
      "The tick is a member of the family Ix\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can pets transmit Lyme disease to humans?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the best diet for a growing puppy?\n",
      "\n",
      "A:\n",
      "\n",
      "I would recommend a low-carb diet.  This is because the puppy is growing at a very rapid rate and the body needs to be able to use the energy it is getting from the food.  The puppy will need to eat a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the best diet for a growing puppy?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the common complications during animal labor?\n",
      "\n",
      "The most common complications during labor are as follows:\n",
      "\n",
      "• Fetal distress\n",
      "\n",
      "• Fetal distress\n",
      "\n",
      "• Fetal distress\n",
      "\n",
      "• Fetal distress\n",
      "\n",
      "• Fetal distress\n",
      "\n",
      "• Fetal distress\n",
      "\n",
      "• Fetal distress\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are the common complications during animal labor?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can I give my dog homemade food instead of kibble?\n",
      "\n",
      "Yes, but you have to be careful. If you give your dog homemade food, you have to make sure that it is safe for your dog to eat. If you give your dog homemade food, you have to make sure that it is safe\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can I give my dog homemade food instead of kibble?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can turtles get respiratory infections?\n",
      "\n",
      "Turtles can get respiratory infections.\n",
      "\n",
      "Turtles can get respiratory infections.\n",
      "\n",
      "Turtles can get respiratory infections.\n",
      "\n",
      "Turtles can get respiratory infections.\n",
      "\n",
      "Turtles can get respiratory infections.\n",
      "\n",
      "Turtles can get\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can turtles get respiratory infections?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When is vomiting in cats considered an emergency?\n",
      "\n",
      "Vomiting is a common sign of an emergency in cats. It is a sign of dehydration, which is a common cause of vomiting in cats.\n",
      "\n",
      "What is the difference between vomiting and diarrhea?\n",
      "\n",
      "Vomiting is the loss of fluid from the\n"
     ]
    }
   ],
   "source": [
    "prompt = \"When is vomiting in cats considered an emergency?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I stop bleeding from a pet's wound?\n",
      "\n",
      "A:\n",
      "\n",
      "The best way to stop bleeding is to stop the bleeding.\n",
      "If you have a pet that is bleeding from a wound, you should immediately stop the bleeding.\n",
      "If you have a pet that is bleeding from a wound, you should\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How do I stop bleeding from a pet's wound?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=64, temperature=1)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
